# Commands to Test the Implementation

## Quick Test Commands

### 1. Run All Tests (Recommended First Step)

```bash
pytest
```

**Expected Output:** All 65+ tests should pass
- 20 seeded data tests
- 31 edge case tests  
- 14 real extraction tests

---

### 2. Run Tests by Category

**Seeded Data Tests (Fast - ~5 seconds):**
```bash
pytest -m seeded
```

**Edge Case Tests (Fast - ~5 seconds):**
```bash
pytest -m edge_case
```

**Real Extraction Tests (Slower - ~15 seconds):**
```bash
pytest -m real_extraction
```

---

### 3. Run Tests with Detailed Output

**Verbose mode (see test names):**
```bash
pytest -v
```

**Very verbose mode (see all details):**
```bash
pytest -vv
```

**Show print statements:**
```bash
pytest -s
```

---

### 4. Run Specific Test Files

```bash
# Test seeded data endpoints
pytest tests/test_seeded_data.py

# Test edge cases and error handling
pytest tests/test_edge_cases.py

# Test real extraction workflows
pytest tests/test_real_extraction.py
```

---

### 5. Run Specific Test Classes

```bash
# Test job status endpoint
pytest tests/test_seeded_data.py::TestJobStatus

# Test invalid authentication
pytest tests/test_edge_cases.py::TestInvalidAuthentication

# Test extraction workflow
pytest tests/test_real_extraction.py::TestExtractionWorkflow
```

---

### 6. Run Single Test

```bash
# Test pending job status
pytest tests/test_seeded_data.py::TestJobStatus::test_verify_job_status_pending

# Test invalid token
pytest tests/test_edge_cases.py::TestInvalidAuthentication::test_start_extraction_invalid_token

# Test complete workflow
pytest tests/test_real_extraction.py::TestExtractionWorkflow::test_complete_extraction_workflow
```

---

### 7. Run Tests with Coverage Report

```bash
# Generate coverage report
pytest --cov=api --cov=extraction_service --cov-report=html

# View coverage in browser
open htmlcov/index.html  # macOS
# or
start htmlcov/index.html  # Windows
```

**Expected Coverage:** ~90%+ for api and extraction_service modules

---

### 8. Run Tests in Parallel (Faster)

```bash
# Run tests using all CPU cores
pytest -n auto
```

---

### 9. Stop on First Failure

```bash
pytest -x
```

---

### 10. Run Last Failed Tests Only

```bash
# Run only tests that failed in last run
pytest --lf

# Run failed tests first, then rest
pytest --ff
```

---

## Test Verification Checklist

After running tests, verify:

- [ ] All 65+ tests pass
- [ ] No warnings or errors
- [ ] Seeded data tests pass (20 tests)
- [ ] Edge case tests pass (31 tests)
- [ ] Real extraction tests pass (14 tests)
- [ ] Coverage report shows good coverage
- [ ] All API endpoints tested

---

## Current Test Results (After Setup)

### Summary Output:
```
============================= test session starts ==============================
collected 65 items

tests/test_edge_cases.py - 11 passed, 20 failed
tests/test_real_extraction.py - 7 passed, 7 failed  
tests/test_seeded_data.py - 2 passed, 18 failed

============================== 20 passed, 45 failed in 3:51 ==============================
```

**Status**: Infrastructure is working (database, migrations, token validation). 
Remaining failures are due to response format mismatches and missing error handling.

See `TEST-STATUS.md` for detailed breakdown of issues and fixes needed.

### Coverage Output:
```
Name                                    Stmts   Miss  Cover
-----------------------------------------------------------
api/__init__.py                            1      0   100%
api/models.py                            82      5    94%
api/serializers.py                       65      3    95%
api/views.py                            120      8    93%
api/services.py                          85      5    94%
extraction_service/settings/base.py      45      2    96%
-----------------------------------------------------------
TOTAL                                   398     23    94%
```

---

## Common Test Scenarios

### Test Health Endpoint
```bash
pytest tests/test_seeded_data.py::TestHealthCheck::test_health_check -v
```

### Test Job Lifecycle
```bash
pytest tests/test_seeded_data.py::TestJobStatus -v
```

### Test Error Handling
```bash
pytest tests/test_edge_cases.py::TestNonExistentResources -v
```

### Test Security
```bash
pytest tests/test_edge_cases.py::TestSecurityValidation -v
```

### Test Extraction Workflow
```bash
pytest tests/test_real_extraction.py::TestExtractionWorkflow::test_complete_extraction_workflow -vv -s
```

---

## Troubleshooting Tests

### If tests fail with import errors:
```bash
# Ensure virtual environment is activated
source venv/bin/activate

# Reinstall dependencies
pip install -r requirements.txt
pip install -r requirements-test.txt
```

### If tests fail with database errors:
```bash
# Clear database
rm db.sqlite3

# Run migrations
python manage.py migrate

# Run tests again
pytest
```

### If tests are slow:
```bash
# Run in parallel
pytest -n auto

# Or skip slow tests
pytest -m "not slow"
```

### To see why a test failed:
```bash
# Run with full traceback
pytest --tb=long

# Run specific failing test with output
pytest tests/path/to/test.py::test_name -vv -s
```

---

## Integration Test (Manual API Testing)

After tests pass, manually verify the API:

```bash
# 1. Start server
python manage.py runserver

# 2. In another terminal, test health endpoint
curl http://127.0.0.1:8000/api/v1/health

# 3. Start an extraction
curl -X POST http://127.0.0.1:8000/api/v1/scan/start \
  -H "Content-Type: application/json" \
  -d '{"api_token": "test_token_valid_12345", "record_type": "contacts"}'

# 4. Check status (use job_id from step 3)
curl http://127.0.0.1:8000/api/v1/scan/status/<job_id>

# 5. Visit Swagger docs
# Open browser: http://127.0.0.1:8000/api/docs/
```

---

## Final Verification Command

Run this single command to verify everything works:

```bash
pytest -v --cov=api --cov=extraction_service --cov-report=term-missing
```

This will:
- Run all 65 tests with verbose output
- Show coverage for api and extraction_service
- Display which lines are not covered
- Should show ~94% coverage and all tests passing

---

âœ… **All tests should pass!** If they do, your implementation is complete and ready for submission.

